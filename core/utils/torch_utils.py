import datetime
import logging
import math
import platform
from contextlib import contextmanager
from pathlib import Path
import subprocess
from copy import deepcopy
import os
from typing import no_type_check_decorator

import torch
import torch.distributed as distributed
import torch.backends.cudnn as cudnn
import torch.nn as nn
import torch.nn.functional as F
import torchvision

try:
    import thop  # è®¡ç®— FLOPS
except ImportError:
    thop = None

logger = logging.getLogger(__name__)


@contextmanager
def torch_distributed_zero_first(local_rank: int):
    """ ç”¨æ¥åŒæ­¥è¿›ç¨‹çš„è£…é¥°å™¨
    åœ¨åˆ†å¸ƒå¼è®­ç»ƒæ—¶ï¼Œè®©å…¶ä»–è¿›ç¨‹ç­‰å¾…ä¸»è¿›ç¨‹æ‰§è¡Œå®Œä¸€äº›æ“ä½œåå†ä¸€èµ·è¿è¡Œ
    """
    if local_rank not in [-1, 0]:
        distributed.barrier()
    yield
    if local_rank == 0:
        distributed.barrier()


def init_torch_seeds(seed=0):
    # Speed-reproducibility tradeoff https://pytorch.org/docs/stable/notes/randomness.html
    torch.manual_seed(seed)
    if seed == 0:  # slower, more reproducible
        cudnn.benchmark, cudnn.deterministic = False, True
    else:  # faster, less reproducible
        cudnn.benchmark, cudnn.deterministic = True, False


def git_describe(path=Path(__file__).parent):  # path must be a directory
    # return human-readable git description, i.e. v5.0-5-g3e25f1e https://git-scm.com/docs/git-describe
    s = f'git -C {path} describe --tags --long --always'
    try:
        return subprocess.check_output(s, shell=True, stderr=subprocess.STDOUT).decode()[:-1]
    except subprocess.CalledProcessError as e:
        return ''  # not a git repository


def date_modified(path=__file__):
    # return human-readable file modification date, i.e. '2021-3-26'
    t = datetime.datetime.fromtimestamp(Path(path).stat().st_mtime)
    return f'{t.year}-{t.month}-{t.day}'


def select_device(device='', batch_size=None):
    # device = 'cpu' or '0' or '0,1,2,3'
    s = f'DeSeNet ğŸš€ {git_describe() or date_modified()} torch {torch.__version__} '  # string
    cpu = device.lower() == 'cpu'
    if cpu:
        os.environ['CUDA_VISIBLE_DEVICES'] = '-1'  # force torch.cuda.is_available() = False
    elif device:  # non-cpu device requested
        os.environ['CUDA_VISIBLE_DEVICES'] = device  # set environment variable
        assert torch.cuda.is_available(), f'CUDA unavailable, invalid device {device} requested'  # check availability

    cuda = not cpu and torch.cuda.is_available()
    if cuda:
        devices = device.split(',') if device else range(torch.cuda.device_count())  # i.e. 0,1,6,7
        n = len(devices)  # device count
        if n > 1 and batch_size:  # check batch_size is divisible by device_count
            assert batch_size % n == 0, f'batch-size {batch_size} not multiple of GPU count {n}'
        space = ' ' * len(s)
        for i, d in enumerate(devices):
            p = torch.cuda.get_device_properties(i)
            s += f"{'' if i == 0 else space}CUDA:{d} ({p.name}, {p.total_memory / 1024 ** 2}MB)\n"  # bytes to MB
    else:
        s += 'CPU\n'

    logger.info(s.encode().decode('ascii', 'ignore') if platform.system() == 'Windows' else s)  # emoji-safe
    return torch.device('cuda:0' if cuda else 'cpu')


def is_parallel(model):
    # å¦‚æœæ¨¡å‹æ˜¯ DP æˆ– DDP ç±»å‹ï¼Œè¿”å› True
    return type(model) in (nn.parallel.DataParallel, nn.parallel.DistributedDataParallel)


def de_parallel(model):
    """å°†ä¸€ä¸ªæ¨¡å‹è§£é™¤å¹¶è¡Œï¼šå¦‚æœæ¨¡å‹æ˜¯ DP æˆ– DDP ç±»å‹ï¼Œè¿”å›ä¸€ä¸ªå• GPU æ¨¡å‹"""
    return model.module if is_parallel(model) else model


def copy_attr(a, b, include=(), exclude=()):
    """æŠŠ b ä¸­ä¸ä»¥ '_' å¼€å¤´çš„å±æ€§å’Œæ–¹æ³•å¤åˆ¶åˆ° aã€‚include ä¸ºç™½åå•ï¼Œexcludeä¸ºé»‘åå•"""
    for k, v in b.__dict__.items():
        if (len(include) and k not in include) or k.startswith('_') or k in exclude:
            continue
        else:
            setattr(a, k, v)


class ModelEMA:
    """ [æ¨¡å‹æŒ‡æ•°ç§»åŠ¨å¹³å‡](https://github.com/rwightman/pytorch-image-models)ä¿æŒæ¨¡å‹ state_dictï¼ˆå‚æ•°å’Œç¼“å†²åŒºï¼‰ ä¸­æ‰€æœ‰å†…å®¹çš„ç§»åŠ¨å¹³å‡å€¼ã€‚
    è¿™æ˜¯ä¸ºäº†å…è®¸[è¿™é‡Œ](https://www.tensorflow.org/api_docs/python/tf/train/ExponentialMovingAverage)ä»‹ç»çš„åŠŸèƒ½ï¼Œ
    ä¸€äº›è®­ç»ƒè®¡åˆ’è¦æƒ³å–å¾—å¥½æˆç»©ï¼Œä¸€ä¸ªå¹³æ»‘ç‰ˆæœ¬çš„æƒé‡æ˜¯ååˆ†å¿…è¦çš„ã€‚
    è¿™ä¸ªç±»å®‰ç½®åœ¨æ¨¡å‹åˆå§‹åŒ–ã€GPUåˆ†é…å’Œåˆ†å¸ƒå¼è®­ç»ƒå°è£…ç­‰ä¸€ç³»åˆ—æ“ä½œä¸­æ˜¯æ•æ„Ÿçš„ã€‚
    """

    def __init__(self, model, decay=0.9999, updates=0):
        """
        model: æ¨¡å‹
        decay: è¡°å‡
        update: EMA æ›´æ–°çš„ä¸ªæ•°ï¼Œé»˜è®¤ä¸º 0
        """

        # åˆ›å»º EMA
        self.ema = deepcopy(de_parallel(model)).eval()  # FP32 EMA
        # if next(model.parameters()).device.type != 'cpu':
        #     self.ema.half()  # FP16 EMA
        self.updates = updates  # number of EMA updates
        self.decay = lambda x: decay * (1 - math.exp(-x / 2000))  # æŒ‡æ•°ä¸‹é™è¡°å‡ (to help early epochs)
        for p in self.ema.parameters():
            p.requires_grad_(False)

    def update(self, model):
        """æ›´æ–° EMA çš„å‚æ•°ä»¬"""
        with torch.no_grad():
            self.updates += 1
            d = self.decay(self.updates)

            msd = de_parallel(model).state_dict()  # æ¨¡å‹çš„ state_dict
            for k, v in self.ema.state_dict().items():
                assert isinstance(v, torch.Tensor)
                if v.dtype.is_floating_point:
                    v *= d
                    v += (1. - d) * msd[k].detach()

    def update_attr(self, model, include=(), exclude=('process_group', 'reducer')):
        """æ›´æ–° EMA attributes"""
        copy_attr(self.ema, model, include, exclude)
